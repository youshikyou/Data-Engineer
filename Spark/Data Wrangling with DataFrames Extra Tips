General functions
We have used the following general functions that are quite similar to methods of pandas dataframes:

select(): returns a new DataFrame with the selected columns
filter(): filters rows using the given condition
where(): is just an alias for filter()
groupBy(): groups the DataFrame using the specified columns, so we can run aggregation on them
sort(): returns a new DataFrame sorted by the specified column(s). By default the second parameter 'ascending' is True.
dropDuplicates(): returns a new DataFrame with unique rows based on all or just a subset of columns
withColumn(): returns a new DataFrame by adding a column or replacing the existing column that has the same name. 
The first parameter is the name of the new column, the second is an expression of how to compute it.
Aggregate functions
Spark SQL provides built-in methods for the most common aggregations such as count(), countDistinct(), avg(), max(), min(), etc. in the pyspark.sql.functions module.
These methods are not the same as the built-in methods in the Python Standard Library, 
where we can find min() for example as well, hence you need to be careful not to use them interchangeably.

In many cases, there are multiple ways to express the same aggregations. For example, if we would like to compute one type of aggregate 
for one or more columns of the DataFrame we can just simply chain the aggregate method after a groupBy(). If we would like to use different functions 
on different columns, agg()comes in handy. For example agg({"salary": "avg", "age": "max"}) computes the average salary and maximum age.

User defined functions (UDF)
In Spark SQL we can define our own functions with the udf method from the pyspark.sql.functions module. The default type of the returned variable for UDFs is string.
If we would like to return an other type we need to explicitly do so by using the different types from the pyspark.sql.types module.

Window functions
Window functions are a way of combining the values of ranges of rows in a DataFrame. When defining the window we can choose how to sort and group 
(with the partitionBy method) the rows and how wide of a window we'd like to use (described by rangeBetween or rowsBetween).


What is Spark Broadcast?
Spark Broadcast variables are secured, read-only variables that get distributed and cached to worker nodes. 
This is helpful to Spark because when the driver sends packets of information to worker nodes, 
it sends the data and tasks attached together which could be a little heavier on the network side. 
Broadcast variables seek to reduce network overhead and to reduce communications. Spark Broadcast variables are used only with Spark Context.


What are Accumulators?
As the name hints, accumulators are variables that accumulate. Because Spark runs in distributed mode, the workers are running in parallel, but asynchronously. 
For example, worker 1 will not be able to know how far worker 2 and worker 3 are done with their tasks. 
With the same analogy, the variables that are local to workers are not going to be shared to another worker unless you accumulate them. 
Accumulators are used for mostly sum operations, like in Hadoop MapReduce, but you can implement it to do otherwise.
