Be careful about unmanaged table and manage table. 
An unmanaged table will always specify the location.It doesn't link table with the data.
https://docs.microsoft.com/en-us/azure/databricks/lakehouse/data-objects#managed-table


You can create a delta table by SQL, dataframe API and deltatable builder.
There are two different ways to create a delta table
One is by path, the other one is by name.
If you use path to create a delta table, you won't see a table in the hive database.
But you can see the data by using path.


******************* SQL to create a delta table *****************
CREATE OR REPLACE TABLE default.people10m (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  ssn STRING,
  salary INT
) USING DELTA

CREATE TABLE IF NOT EXISTS default.people10m (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  ssn STRING,
  salary INT
) USING DELTA


-- Create or replace table with path
CREATE OR REPLACE TABLE delta.`/tmp/delta/people10m` (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  ssn STRING,
  salary INT
) USING DELTA


*************** DataFrameWriter API to create a delta table ********************

# Create table in the metastore using DataFrame's schema and write data to it
df.write.format("delta").saveAsTable("default.people10m")

# Create or replace partitioned table with path using DataFrame's schema and write/overwrite data to it
df.write.format("delta").mode("overwrite").save("/tmp/delta/people10m")

*****************************
# example1,
(
    df_read
    .write
    .format("delta")
    .mode("append")
    .option("mergeSchema", "true")
    .partitionBy("sourcefolderdate")
    .save(path)
)
this creates a delta table data in the path,
in the SQL space if you want to get the data, you need 
select * from delta.`path`
You don't have the table in the database!!!

You need spark.read.format.load(path) to get the dataframe.
*************************************

************************************
# example2,
(
    df_read
    .write
    .format("delta")
    .mode("append")
    .option("mergeSchema", "true")
    .partitionBy("sourcefolderdate")
    .saveAsTable("default.raw_data")
)

 this creates a delta table in the default database.
 In the SQL space you need
 select * from defualt.raw_data
 
 You need spark.table("default.raw_data") to get the dataframe

*************** DeltaTableBuilder API to create a delta table ***************
# Create table in the metastore
DeltaTable.createIfNotExists(spark) \
  .tableName("default.people10m") \
  .addColumn("id", "INT") \
  .addColumn("firstName", "STRING") \
  .addColumn("middleName", "STRING") \
  .addColumn("lastName", "STRING", comment = "surname") \
  .addColumn("gender", "STRING") \
  .addColumn("birthDate", "TIMESTAMP") \
  .addColumn("ssn", "STRING") \
  .addColumn("salary", "INT") \
  .execute()

# Create or replace table with path and add properties
DeltaTable.createOrReplace(spark) \
  .addColumn("id", "INT") \
  .addColumn("firstName", "STRING") \
  .addColumn("middleName", "STRING") \
  .addColumn("lastName", "STRING", comment = "surname") \
  .addColumn("gender", "STRING") \
  .addColumn("birthDate", "TIMESTAMP") \
  .addColumn("ssn", "STRING") \
  .addColumn("salary", "INT") \
  .property("description", "table with people data") \
  .location("/tmp/delta/people10m") \
  .execute()
  
# one lazy way to define the columns  
tbl =  (
        DeltaTable
        .createIfNotExists(spark_session)
        .addColumns(df_read.schema)
        .partitionedBy("date")
        .location(path)
        )
    if hive_tablename is not None:
        tbl = tbl.tableName(hive_tablename)
    tbl.execute()

********Extract Raw Events From JSON Files***********
create table if not exists events_json
(key BINARY,offset LONG, partition INTEGER, timestamp LONG, topic STRING, value BINARY)
using json
options(path = "${da.paths.datasets}/raw/events-kafka")

***********Insert Raw Events Into Delta Table***********
insert into events_raw
select * from events_json

***********Create Delta Table from a Query***********
create or replace table item_lookup
as select * from parquet.`${da.paths.datasets}/raw/item-lookup`


***********Add a Table Constraint***********
ALTER TABLE purchase_dates ADD CONSTRAINT valid_date CHECK (date > '2020-01-01');


***********Enrich Tables with Additional Options and Metadata***********
CREATE OR REPLACE TABLE users_pii
COMMENT "Contains PII"
LOCATION "${da.paths.working_dir}/tmp/users_pii"
PARTITIONED BY (first_touch_date)
AS
  SELECT *, 
    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, 
    current_timestamp() updated,
    input_file_name() source_file
  FROM parquet.`${da.paths.datasets}/raw/users-historical/`;
  
***********Cloning Delta Lake Tables***********
CREATE OR REPLACE TABLE purchases_clone
DEEP CLONE purchases

CREATE OR REPLACE TABLE purchases_shallow_clone
SHALLOW CLONE purchases

***********Load Incrementally***********
COPY INTO sales
FROM "${da.paths.datasets}/raw/sales-30m"
FILEFORMAT = PARQUET
