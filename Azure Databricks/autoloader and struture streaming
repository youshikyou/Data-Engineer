******************Configure Streaming Read******************
This lab uses a collection of customer-related CSV data from DBFS found in /databricks-datasets/retail-org/customers/.
Read this data using Auto Loader using its schema inference (use customers_checkpoint_path to store the schema info).
Create a streaming temporary view called customers_raw_temp.


customers_checkpoint_path = f"{DA.paths.checkpoints}/customers"

(spark
  .readStream
  .format("cloudFiles")
  .option("cloudFiles.format","csv")
  .option("cloudFiles.schemalocation",customers_checkpoint_path)
  .load("/databricks-datasets/retail-org/customers/")
  .createOrReplaceTempView("customers_raw_temp"))
  
  
******************Write aggregated data to a Delta table******************
customers_count_checkpoint_path = f"{DA.paths.checkpoints}/customers_count"

query = (spark.table("customer_count_by_state_temp")
         .writeStream
         .format("delta")
         .option("checkpointLocation",customers_checkpoint_path)
         .outputMode("complete")
         .table("customer_count_by_state")
        )

******************A whole pipleline for struture streaming******************
1.ingest data 
Read this data using Auto Loader using its schema inference (use customers_checkpoint_path to store the schema info).
Stream the raw data to a Delta table called bronze.

customers_checkpoint_path = f"{DA.paths.checkpoints}/customers"
query = (spark
  .readStream
  .format("cloudFiles")
  .option("cloudFiles.format","csv")
  .option("cloudFiles.schemalocation",customers_checkpoint_path)
  .load("/databricks-datasets/retail-org/customers/")
  .writeStream
  .format("delta")
  .option("checkpointLocation",customers_checkpoint_path)
  .outputMode("append")
  .table("bronze")
)

2. create a streaming temporary view into the bronze table, so that we can perform transforms using SQL.

(spark
  .readStream
  .table("bronze")
  .createOrReplaceTempView("bronze_temp"))
  
3. Clean and enhance data

CREATE OR REPLACE TEMPORARY VIEW bronze_enhanced_temp AS
SELECT
  *,current_timestamp() as receipt_time,input_file_name() as source_file
  from bronze_temp
  where postcode>0
  
4.Silver table

silver_checkpoint_path = f"{DA.paths.checkpoints}/silver"
query = (spark.table("bronze_enhanced_temp")
  .writeStream
  .format("delta")
  .option("checkpointlocation",silver_checkpoint_path)
  .outputMode("append")
  .table("silver"))

5. create a streaming temporary view into the silver table, so that we can perform business-level using SQL.

(spark
  .readStream
  .table("silver")
  .createOrReplaceTempView("silver_temp"))
  
6. business oriented data

CREATE OR REPLACE TEMPORARY VIEW customer_count_temp AS
SELECT
 state,
 count(state) as customer_count 
 from silver_temp
 group by state

7.Gold table

customers_count_checkpoint_path = f"{DA.paths.checkpoints}/customers_counts"
query = (spark
  .table("customer_count_temp")
  .writeStream
  .format("delta")
  .option("checkpointlocation",silver_checkpoint_path)
  .outputMode("complete")
  .table("gold_customer_count_by_state"))

8. query the result
  SELECT * FROM gold_customer_count_by_state 
